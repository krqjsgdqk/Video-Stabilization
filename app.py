import sys
import asyncio
import os

# ==========================================
# 1. SYSTEM CONFIG & ERROR HANDLING
# ==========================================
# Thi·∫øt l·∫≠p event loop policy tr√™n Windows ƒë·ªÉ tr√°nh l·ªói Proactor/asyncio
if sys.platform == 'win32':
    try:
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    except Exception:
        pass

# NgƒÉn Gradio c·ªë t·∫°o share link qua Internet
os.environ.setdefault('GRADIO_SHARE', 'False')

import gradio as gr
import cv2
import numpy as np
import matplotlib.pyplot as plt
from skimage.metrics import structural_similarity as ssim
import tempfile

# ==========================================
# 2. CORE ALGORITHMS
# ==========================================

def median_filter(curve, kernel_size=5):
    """L·ªçc median lo·∫°i b·ªè spike/outlier trong signal 1D."""
    if kernel_size <= 1 or len(curve) == 0:
        return curve
    pad = kernel_size // 2
    curve_pad = np.pad(curve, (pad, pad), 'edge')
    out = np.empty_like(curve)
    for i in range(len(curve)):
        out[i] = np.median(curve_pad[i:i+kernel_size])
    return out

def moving_average(curve, radius):
    """
    L√†m m∆∞·ª£t d·ªØ li·ªáu (Smoothing Trajectory).
    S·ª≠ d·ª•ng k·ªπ thu·∫≠t t√≠ch ch·∫≠p (Convolution) v·ªõi c·ª≠a s·ªï tr∆∞·ª£t.
    """
    window_size = 2 * radius + 1
    # T·∫°o b·ªô l·ªçc (kernel) trung b√¨nh
    f = np.ones(window_size) / window_size
    # Padding bi√™n ƒë·ªÉ gi·ªØ nguy√™n k√≠ch th∆∞·ªõc m·∫£ng sau khi l·ªçc
    curve_pad = np.pad(curve, (radius, radius), 'edge')
    # √Åp d·ª•ng t√≠ch ch·∫≠p
    curve_smoothed = np.convolve(curve_pad, f, mode='same')
    # C·∫Øt b·ªè ph·∫ßn padding
    return curve_smoothed[radius:-radius]

def compute_metrics_safe(img1, img2):
    """
    T√≠nh PSNR v√† SSIM tr√™n ·∫£nh thu nh·ªè ƒë·ªÉ t·ªëi ∆∞u hi·ªáu nƒÉng.
    """
    # Resize v·ªÅ chi·ªÅu r·ªông 320px ƒë·ªÉ t√≠nh to√°n nhanh
    h, w = img1.shape[:2]
    scale = 320 / w
    new_size = (320, int(h * scale))
    
    s1 = cv2.resize(img1, new_size)
    s2 = cv2.resize(img2, new_size)
    
    # T√≠nh PSNR
    psnr_val = cv2.PSNR(s1, s2)
    
    # T√≠nh SSIM (c·∫ßn ·∫£nh x√°m)
    g1 = cv2.cvtColor(s1, cv2.COLOR_BGR2GRAY)
    g2 = cv2.cvtColor(s2, cv2.COLOR_BGR2GRAY)
    ssim_val = ssim(g1, g2, data_range=g2.max() - g2.min())
    
    return psnr_val, ssim_val   

# ==========================================
# 3. MAIN PROCESSING PIPELINE
# ==========================================

def process_video(input_path, tech_detector, smoothing_radius, fast_mode=False, progress=gr.Progress()):
    """
    H√†m x·ª≠ l√Ω ch√≠nh: Nh·∫≠n video -> ·ªîn ƒë·ªãnh -> Xu·∫•t video & Metrics
    """
    if input_path is None:
        return None, None
    
    # --- SETUP INPUT ---
    cap = cv2.VideoCapture(input_path)
    n_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    
    # --- SETUP OUTPUT ---
    tmpfile = tempfile.NamedTemporaryFile(delete=False, suffix='.mp4')
    output_temp = tmpfile.name
    tmpfile.close()
    # Codec mp4v t∆∞∆°ng th√≠ch t·ªët v·ªõi OpenCV c∆° b·∫£n
    # Choose metric frequency to reduce load in fast_mode
    metric_freq = 4 if fast_mode else 2
    
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(output_temp, fourcc, fps, (w, h))

    # --- SETUP DETECTOR (adjust for speed_mode) ---
    if tech_detector == "ORB":
        nfeat = 1000 if fast_mode else 5000
        detector = cv2.ORB_create(nfeatures=nfeat)
        matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)
    else: # SIFT
        # SIFT is slower; if fast_mode, use fewer keypoints by limiting detection later
        detector = cv2.SIFT_create()
        matcher = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)

    # Resize factor used only for feature detection/matching when fast_mode
    if fast_mode:
        scale = 0.5 if max(w, h) > 800 else 1.0
    else:
        scale = 1.0

    # ==========================================
    # PASS 1: MOTION ESTIMATION (∆Ø·ªõc l∆∞·ª£ng chuy·ªÉn ƒë·ªông)
    # ==========================================
    transforms = [] # L∆∞u tr·ªØ dx, dy, da cho t·ª´ng frame
    
    _, prev_frame = cap.read()
    prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)
    
    # Pre-pad transform ƒë·∫ßu ti√™n l√† 0
    transforms.append([0, 0, 0]) 

    progress(0, desc="Giai ƒëo·∫°n 1/2: Ph√¢n t√≠ch chuy·ªÉn ƒë·ªông...")
    
    for i in range(n_frames - 2):
        success, curr_frame = cap.read()
        if not success: break
        
        curr_gray = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)
        # use resized gray for faster feature detection in fast mode
        if scale != 1.0:
            prev_gray_small = cv2.resize(prev_gray, (0,0), fx=scale, fy=scale)
            curr_gray_small = cv2.resize(curr_gray, (0,0), fx=scale, fy=scale)
        else:
            prev_gray_small = prev_gray
            curr_gray_small = curr_gray
        
        # 1. Detect Features (on small images if fast_mode)
        kp1, des1 = detector.detectAndCompute(prev_gray_small, None)
        kp2, des2 = detector.detectAndCompute(curr_gray_small, None)
        
        # M·∫∑c ƒë·ªãnh: kh√¥ng chuy·ªÉn ƒë·ªông
        delta_x, delta_y, delta_angle = 0, 0, 0
        
        if des1 is not None and des2 is not None:
            # 2. Match Features
            matches = matcher.match(des1, des2)
            # L·ªçc l·∫•y top matches (gi·∫£m khi fast_mode)
            max_matches = 200 if fast_mode else 500
            matches = sorted(matches, key=lambda x: x.distance)[:max_matches]
            
            if len(matches) > 10:
                # Tr√≠ch xu·∫•t t·ªça ƒë·ªô ƒëi·ªÉm
                src_pts = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)
                dst_pts = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)
                # If detection was done on scaled images, map points back to original coords
                if scale != 1.0:
                    src_pts = src_pts / scale
                    dst_pts = dst_pts / scale
                
                # 3. Estimate Transform (v·ªõi RANSAC)
                # estimateAffinePartial2D t·ªët h∆°n estimateAffine2D cho video quay tay
                # v√¨ n√≥ h·∫°n ch·∫ø bi·∫øn d·∫°ng (ch·ªâ t·ªãnh ti·∫øn + xoay + scale)
                m_trans, _ = cv2.estimateAffinePartial2D(src_pts, dst_pts)
                
                if m_trans is not None:
                    # Tr√≠ch xu·∫•t tham s·ªë t·ª´ ma tr·∫≠n 2x3
                    delta_x = m_trans[0, 2]
                    delta_y = m_trans[1, 2]
                    delta_angle = np.arctan2(m_trans[1, 0], m_trans[0, 0])

        transforms.append([delta_x, delta_y, delta_angle])
        prev_gray = curr_gray

    # ==========================================
    # TRAJECTORY SMOOTHING (L√†m m∆∞·ª£t qu·ªπ ƒë·∫°o)
    # ==========================================
    transforms = np.array(transforms)
    
    # T√≠nh qu·ªπ ƒë·∫°o t√≠ch l≈©y (Cumulative Trajectory)
    trajectory = np.cumsum(transforms, axis=0)
    
    # L√†m m∆∞·ª£t qu·ªπ ƒë·∫°o v·ªõi median filter tr∆∞·ªõc (lo·∫°i spike)
    smoothed_trajectory = np.copy(trajectory)
    for i in range(3): # 0:x, 1:y, 2:angle
        smoothed_trajectory[:, i] = median_filter(trajectory[:, i], kernel_size=5)
        smoothed_trajectory[:, i] = moving_average(smoothed_trajectory[:, i], smoothing_radius)
        
    # T√≠nh ƒë·ªô l·ªách c·∫ßn b√π tr·ª´ (Correction)
    difference = smoothed_trajectory - trajectory
    transforms_smooth = transforms + difference

    # ==========================================
    # PASS 2: RENDERING & METRICS (Xu·∫•t video & T√≠nh to√°n)
    # ==========================================
    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
    
    psnr_history = []
    ssim_history = []
    
    progress(0.5, desc="Giai ƒëo·∫°n 2/2: ·ªîn ƒë·ªãnh h√¨nh ·∫£nh & Render...")

    for i in range(len(transforms_smooth) - 1):
        success, frame = cap.read()
        if not success: break
        
        # L·∫•y tham s·ªë transform ƒë√£ l√†m m∆∞·ª£t
        dx, dy, da = transforms_smooth[i]
        
        # T·∫°o ma tr·∫≠n xoay quanh t√¢m khung h√¨nh (t·ªët h∆°n xoay quanh g·ªëc)
        center = (w / 2.0, h / 2.0)
        angle_deg = (da * 180.0) / np.pi
        m = cv2.getRotationMatrix2D(center, angle_deg, 1.0)
        # Th√™m ph·∫ßn t·ªãnh ti·∫øn
        m[0, 2] += dx
        m[1, 2] += dy
        
        # Warp ·∫£nh (Stabilize) v·ªõi BORDER_REPLICATE ƒë·ªÉ tr√°nh vi·ªÅn ƒëen
        frame_stabilized = cv2.warpAffine(frame, m, (w, h), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REPLICATE)
        
        # FIX BORDER: Zoom nh·∫π 4% ƒë·ªÉ c·∫Øt b·ªè vi·ªÅn do warp
        scale_zoom = 1.04
        M_zoom = cv2.getRotationMatrix2D((w/2, h/2), 0, scale_zoom)
        frame_stabilized = cv2.warpAffine(frame_stabilized, M_zoom, (w, h), borderMode=cv2.BORDER_REPLICATE)
        
        # Ghi video
        out.write(frame_stabilized)
        
        # T√≠nh Metrics (gi·∫£m t·∫ßn su·∫•t khi fast_mode ƒë·ªÉ tƒÉng t·ªëc)
        if i % metric_freq == 0:
            p, s = compute_metrics_safe(frame, frame_stabilized)
            psnr_history.append(p)
            ssim_history.append(s)
        else:
            # D√πng l·∫°i gi√° tr·ªã c≈© ƒë·ªÉ gi·ªØ array li√™n t·ª•c
            if psnr_history:
                psnr_history.append(psnr_history[-1])
                ssim_history.append(ssim_history[-1])
            else:
                psnr_history.append(0)
                ssim_history.append(0)

    cap.release()
    out.release()
    
    # ==========================================
    # VISUALIZATION (V·∫Ω bi·ªÉu ƒë·ªì)
    # ==========================================
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
    
    ax1.plot(psnr_history, color='tab:blue')
    ax1.set_title('PSNR (ƒê·ªô nhi·ªÖu t√≠n hi·ªáu)')
    ax1.set_xlabel('Frame')
    ax1.set_ylabel('dB')
    ax1.grid(True, alpha=0.3)
    
    ax2.plot(ssim_history, color='tab:orange')
    ax2.set_title('SSIM (ƒê·ªô t∆∞∆°ng ƒë·ªìng c·∫•u tr√∫c)')
    ax2.set_xlabel('Frame')
    ax2.set_ylabel('Index (0-1)')
    ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()

    # Ensure the temp file is readable by other processes on Windows
    try:
        os.chmod(output_temp, 0o666)
    except Exception:
        pass

    return output_temp, fig

# ==========================================
# 4. GRADIO INTERFACE
# ==========================================
with gr.Blocks(title="Professional Video Stabilizer") as demo:
    gr.Markdown(
        """
        # üé• Video Stabilization System (Detailed Implementation)
        H·ªá th·ªëng ·ªïn ƒë·ªãnh video s·ª≠ d·ª•ng pipeline: **Feature Matching -> Motion Estimation (RANSAC) -> Trajectory Smoothing**.
        """
    )
    
    with gr.Row():
        with gr.Column(scale=1):
            input_video = gr.Video(label="Input Video", sources=["upload"])
            
            with gr.Group():
                gr.Markdown("### ‚öôÔ∏è C·∫•u h√¨nh thu·∫≠t to√°n (Enhanced)")
                rad_detector = gr.Radio(["ORB", "SIFT"], value="ORB", label="Ph∆∞∆°ng ph√°p tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng")
                slider_smooth = gr.Slider(15, 80, value=35, step=5, label="B√°n k√≠nh l√†m m∆∞·ª£t (Smoothing Radius)")
                fast_mode = gr.Checkbox(value=False, label="Fast Mode (tƒÉng t·ªëc, gi·∫£m ch·∫•t l∆∞·ª£ng)")
                gr.Info("‚ú® C·∫£i ti·∫øn: Median filter + nfeatures 5000 + Center rotation + BORDER_REPLICATE")
            
            btn_run = gr.Button("üöÄ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω", variant="primary")
            
        with gr.Column(scale=1):
            output_video = gr.Video(label="K·∫øt qu·∫£ ·ªîn ƒë·ªãnh (Stabilized)")
            plot_result = gr.Plot(label="Ph√¢n t√≠ch ch·∫•t l∆∞·ª£ng (PSNR/SSIM)")

    btn_run.click(
        fn=process_video,
        inputs=[input_video, rad_detector, slider_smooth, fast_mode],
        outputs=[output_video, plot_result]
    )

if __name__ == "__main__":
    # share=False ƒë·ªÉ ch·∫°y local nhanh h∆°n
    demo.launch(share=False)